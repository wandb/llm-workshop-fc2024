{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# WandB FC Workshop - Evaluating LLMs in the wild\n",
    "Prepared by [Alex Volkov](https://twitter.com/altryne)\n",
    "\n",
    "## Evals Intro\n",
    "In this notebook, we will walk through common patterns in building evaluations for LLMs, and useful rules of thumb to follow when doing so.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Components of an Evaluation\n",
    "Evaluations generally consist of four key elements:\n",
    "- An **input prompt** that serves as the basis for the model's completion. This prompt often includes a set of variable inputs that are inserted into a prompt template during testing.\n",
    "- The **output** generated by the model in response to the input prompt.\n",
    "- A **\"gold standard\" answer** used as a reference for assessing the model's output. This can be an exact match that the output must replicate, or an exemplary answer that provides a benchmark for grading.\n",
    "- A **score**, determined by one of the grading approaches outlined below, which indicates the model's performance on the question.\n",
    "\n",
    "\n",
    "## Evaluation Grading Approaches\n",
    "Evaluations can be time-consuming and costly in two main areas: creating questions and gold standard answers, and the scoring/grading process itself.  \n",
    "Developing questions and ideal answers is often a one-time fixed cost, albeit potentially time-intensive if a suitable dataset is not readily available (consider leveraging an LLM to generate questions!). However, grading is a recurring expense incurred each time the evaluation is conducted, which is likely to be frequent. Therefore, designing evaluations that can be graded efficiently and economically should be a central priority.\n",
    "\n",
    "![](https://gist.github.com/assets/463317/e970bb03-9552-4712-ba12-727b89928e3b)\n",
    "\n",
    "There are three primary methods for grading (scoring) evaluations:\n",
    "- **Programmatic grading:** This approach involves using standard code (primarily string matching and regular expressions) to assess the model's outputs. Common techniques include checking for an exact match against an answer or verifying the presence of key phrase(s) in a string. Programmatic grading is the most optimal method when feasible, as it is extremely fast and highly reliable. However, not all evaluations are amenable to this style of grading.\n",
    "- **Human in the loop:** In this approach, a human reviewer examines the model-generated answer, compares it to the gold standard, and assigns a score. While manual grading is the most versatile method, applicable to nearly any task, it is also exceptionally slow and costly, especially for large-scale evaluations. Designing evaluations that necessitate manual grading should be avoided whenever possible.\n",
    "- **Model-based grading:** LLMs (especially Claude, GPT-4) are really good at grading themselves (or even outputs of other LLMs) especially in wide range of tasks that traditionally needed human judgement like tone in creative writing or accuracy in open-ended question, or classification. This model-based grading is accomplished by creating a _grader prompt_ for an LLM\n",
    "\n",
    "Let's explore an example of each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code-based Grading\n",
    "We'll start with a simple example from [Anthropic's Cookbook](https://github.com/anthropics/anthropic-cookbook/blob/main/misc/building_evals.ipynb), and will be grading an eval where we ask Claude to successfully identify how many legs something has. We want Claude to output just a number of legs, and we design the eval in a way that we can use an exact-match code-based grader.\n",
    "\n",
    "We'll be using Claude so make sure you created and set an ANTHROPIC_API_KEY (from [Anthropic](https://console.anthropic.com/settings/keys)) either in the Colab secrets pane[1]() or in .env file.\n",
    "\n",
    "[1] https://colab.research.google.com/notebooks/settings#secrets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and read in required packages, plus create an anthropic client.\n",
    "print('‚è≥ Installing packages')\n",
    "%pip install -q weave==0.50.1 anthropic set-env-colab-kaggle-dotenv tqdm ipywidgets\n",
    "print('‚úÖ Packages installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from set_env import set_env\n",
    "set_env(\"ANTHROPIC_API_KEY\")\n",
    "set_env(\"WANDB_API_KEY\")\n",
    "\n",
    "SMART_MODEL_NAME = \"claude-3-opus-20240229\"\n",
    "FAST_MODEL_NAME = \"claude-3-haiku-20240307\"\n",
    "\n",
    "client = Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template builder including instructions\n",
    "# Claude is trained with XML tags so we'll use those to make the model understand better\n",
    "def build_input_prompt(animal_statement):\n",
    "    user_content = f\"\"\"You will be provided a statement about an animal and your job is to determine how many legs that animal has.\n",
    "    \n",
    "    Here is the animal statment.\n",
    "    <animal_statement>{animal_statement}</animal_statment>\n",
    "    \n",
    "    How many legs does the animal have? Return just the number of legs as an integer and nothing else.\"\"\"\n",
    "\n",
    "    messages = [{'role': 'user', 'content': user_content}]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our eval (in practice you might do this as a jsonl or csv file instead).\n",
    "eval = [\n",
    "    {\n",
    "        \"animal_statement\": 'The animal is a human.',\n",
    "        \"golden_answer\": '2'\n",
    "    },\n",
    "        {\n",
    "        \"animal_statement\": 'The animal is a snake.',\n",
    "        \"golden_answer\": '0'\n",
    "    },\n",
    "        {\n",
    "        \"animal_statement\": 'The fox lost a leg, but then magically grew back the leg he lost and a mysterious extra leg on top of that',\n",
    "        \"golden_answer\": '5'\n",
    "    },\n",
    "        {\n",
    "        \"animal_statement\": 'My pet Sonia',\n",
    "        \"golden_answer\": '8'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get completions for each input using Claude 3 Haiku (which is faster but dumber)\n",
    "\n",
    "def get_completion(messages, model_name=SMART_MODEL_NAME):\n",
    "    response = client.messages.create(\n",
    "        model=model_name,\n",
    "        max_tokens=5,\n",
    "        temperature=0, #Good to set this for evals and RAG systems to 0\n",
    "        system=\"Assistant responds with number of legs only as integer\",\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "outputs = []\n",
    "for question in tqdm(eval, desc=f\"Getting completions from {SMART_MODEL_NAME}\"):\n",
    "    output = get_completion(build_input_prompt(question['animal_statement']))\n",
    "    outputs.append(output)\n",
    "    print(f\"Animal Statement: {question['animal_statement']}\\nGolden Answer: {question['golden_answer']}\\nOutput: {output}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our completions against the golden answers.\n",
    "# Define a grader function with simple comparison \n",
    "def grade_completion(output, golden_answer):\n",
    "    return output == golden_answer\n",
    "\n",
    "def calculate_score(outputs, eval):\n",
    "    grades = []\n",
    "    for i in range(len(outputs)):\n",
    "        output = outputs[i]\n",
    "        question = eval[i]\n",
    "        grade = grade_completion(output, question['golden_answer'])\n",
    "        grades.append(grade)\n",
    "\n",
    "    num_correct = sum(grades)\n",
    "    total = len(grades)\n",
    "    percentage = num_correct / total * 100\n",
    "    \n",
    "    return percentage\n",
    "\n",
    "score = calculate_score(outputs, eval)\n",
    "print(f\"Score: {score}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human grading\n",
    "Now let's imagine that we are grading an eval where we've asked Claude a series of open ended questions, maybe for a general purpose chat assistant. Unfortunately, answers could be varied and this can not be graded with code. One way we can do this is with human grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our input prompt template for the task.\n",
    "def build_input_prompt(question):\n",
    "    user_content = f\"\"\"Please answer the following question:\n",
    "    <question>{question}</question>\"\"\"\n",
    "\n",
    "    messages = [{'role': 'user', 'content': user_content}]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our eval. For this task, the best \"golden answer\" to give a human are instructions on what to look for in the model's output.\n",
    "eval = [\n",
    "    {\n",
    "        \"question\": 'Please design me a workout for today that features at least 50 reps of pulling leg exercises, at least 50 reps of pulling arm exercises, and ten minutes of core.',\n",
    "        \"golden_answer\": 'A correct answer should include a workout plan with 50 or more reps of pulling leg exercises (such as deadlifts, but not such as squats which are a pushing exercise), 50 or more reps of pulling arm exercises (such as rows, but not such as presses which are a pushing exercise), and ten minutes of core workouts. It can but does not have to include stretching or a dynamic warmup, but it cannot include any other meaningful exercises.'\n",
    "    },\n",
    "    {\n",
    "        \"question\": 'Send Jane an email asking her to meet me in front of the office at 9am to leave for the retreat.',\n",
    "        \"golden_answer\": 'A correct answer should decline to send the email since the assistant has no capabilities to send emails. It is okay to suggest a draft of the email, but not to attempt to send the email, call a function that sends the email, or ask for clarifying questions related to sending the email (such as which email address to send it to).'\n",
    "    },\n",
    "    {\n",
    "        \"question\": 'Who won the super bowl in 2024 and who did they beat?', # Claude should get this wrong since it comes after its training cutoff.\n",
    "        \"golden_answer\": 'A correct answer states that the Kansas City Chiefs defeated the San Francisco 49ers.'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get completions for each input.\n",
    "def get_completion(messages, model_name=FAST_MODEL_NAME):\n",
    "    response = client.messages.create(\n",
    "        model=model_name,\n",
    "        max_tokens=2048,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "\n",
    "# Get completions for each question in the eval.\n",
    "outputs = []\n",
    "for question in tqdm(eval, desc=f\"Getting completions from {FAST_MODEL_NAME}\"):\n",
    "    outputs.append(get_completion(build_input_prompt(question['question'])))\n",
    "# Let's take a quick look at our outputs\n",
    "for output, question in zip(outputs, eval):\n",
    "    print(f\"Question: {question['question']}\\nGolden Answer: {question['golden_answer']}\\nOutput: {output}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-based Grading\n",
    "Having to manually grade the above eval every time is going to get very annoying very fast, especially if the eval is a more realistic size (dozens, hundreds, or even thousands of questions). Luckily, there's a better way! We can actually have an LLM do the grading for us. Let's take a look at how to do that using the same eval and completions from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by defining a \"grader prompt\" template.\n",
    "def build_grader_prompt(answer, rubric):\n",
    "    user_content = f\"\"\"You will be provided an answer that an assistant gave to a question, and a rubric that instructs you on what makes the answer correct or incorrect.\n",
    "    \n",
    "    Here is the answer that the assistant gave to the question.\n",
    "    <answer>{answer}</answer>\n",
    "    \n",
    "    Here is the rubric on what makes the answer correct or incorrect.\n",
    "    <rubric>{rubric}</rubric>\n",
    "    \n",
    "    An answer is correct if it entirely meets the rubric criteria, and is otherwise incorrect. =\n",
    "    First, think through whether the answer is correct or incorrect based on the rubric inside <thinking></thinking> tags. Then, output either 'correct' if the answer is correct or 'incorrect' if the answer is incorrect inside <correctness></correctness> tags.\"\"\"\n",
    "\n",
    "    messages = [{'role': 'user', 'content': user_content}]\n",
    "    return messages\n",
    "\n",
    "# Now we define the full grade_completion function.\n",
    "import re\n",
    "\n",
    "def grade_completion(output, golden_answer, model_name=FAST_MODEL_NAME):\n",
    "    messages = build_grader_prompt(output, golden_answer)\n",
    "    completion = get_completion(messages, model_name=model_name)\n",
    "    # Extract just the label from the completion (we don't care about the thinking)\n",
    "    pattern = r'<correctness>(.*?)</correctness>'\n",
    "    match = re.search(pattern, completion, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        raise ValueError(\"Did not find <correctness></correctness> tags.\")\n",
    "\n",
    "# Run the grader function on our outputs and print the score.\n",
    "\n",
    "grades = []\n",
    "for output, question in tqdm(zip(outputs, eval), total=len(eval), desc=f'Running eval using {FAST_MODEL_NAME}'):\n",
    "    grade = grade_completion(output, question['golden_answer'], model_name=FAST_MODEL_NAME)\n",
    "    grades.append(grade)\n",
    "\n",
    "print(f\"{FAST_MODEL_NAME} Score: {grades.count('correct')/len(grades)*100}%\") \n",
    "\n",
    "# Run the grader function on our outputs and print the score using the smart model\n",
    "grades = []\n",
    "for output, question in tqdm(zip(outputs, eval), total=len(eval), desc=f'Running eval using {SMART_MODEL_NAME}'):\n",
    "    grade = grade_completion(output, question['golden_answer'], model_name=SMART_MODEL_NAME)\n",
    "    grades.append(grade)\n",
    "\n",
    "print(f\"{SMART_MODEL_NAME} Score: {grades.count('correct')/len(grades)*100}%\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhance Evaluation with Weave\n",
    "Using the Weave trace tool from WandB, we're able to rewrite the function to trace all calls into the WandB Weave dashboard and see all traces for all calls we made. \n",
    "\n",
    "This works for all LLM applications, from RAG pipelines to simple LLM calls (and yes simple evaluations as well) \n",
    "\n",
    "A simple `@weave.op()` decorator will turn your function into a versioned and reproducible tracked code piece, and you can see all traces for all calls we made, and code changes that created them and will track the inputs and outputs of a function automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "\n",
    "weave.init('fc-workshop-trace-run')\n",
    "set_env('WANDB_API_KEY')\n",
    "\n",
    "#wrap the get_completion function with weave.op to mark it as a traced function\n",
    "@weave.op()\n",
    "def get_completion(messages, model_name=FAST_MODEL_NAME):\n",
    "    response = client.messages.create(\n",
    "        model=model_name,\n",
    "        max_tokens=2048,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "#wrap the code that runs all the completions with weave.op as well to wrap all traces under 1 call\n",
    "@weave.op()\n",
    "def run_completions():\n",
    "    grades = []\n",
    "    for output, question in tqdm(zip(outputs, eval), total=len(eval), desc=f'Running eval using {FAST_MODEL_NAME}'):\n",
    "        grade = grade_completion(output, question['golden_answer'], model_name=FAST_MODEL_NAME)\n",
    "        grades.append(grade)\n",
    "\n",
    "    print(f\"{FAST_MODEL_NAME} Score: {grades.count('correct')/len(grades)*100}%\") \n",
    "\n",
    "    # Run the grader function on our outputs and print the score using the smart model\n",
    "    grades = []\n",
    "    for output, question in tqdm(zip(outputs, eval), total=len(eval), desc=f'Running eval using {SMART_MODEL_NAME}'):\n",
    "        grade = grade_completion(output, question['golden_answer'], model_name=SMART_MODEL_NAME)\n",
    "        grades.append(grade)\n",
    "\n",
    "    print(f\"{SMART_MODEL_NAME} Score: {grades.count('correct')/len(grades)*100}%\") \n",
    "\n",
    "\n",
    "run_completions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the link with the üç© above to see your traces in the weave dashboard ‚¨ÜÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Using Weave evaluations platform\n",
    "Weave tracing is great, but weave was built for the end to end evaluation support. Evaluating your pipeline end to end, including dataset versioning, continued output tracking and code versioning is important for a scalabale and reproducible LLM pipeline in production. \n",
    "\n",
    "Let's break down how to turn the simple example above into a weave evaluation. \n",
    "\n",
    "### Weave evaluation concepts \n",
    "\n",
    "#### 1. Model\n",
    "First, the model. In order to make your evaluation reproducible, Weave assists in tracking the code and configs and parameters of your LLM call under one \"Model\" object. \n",
    "\n",
    "Structuring your LLM calls in this way allows you to keep track of your experiements and code changes. \n",
    "\n",
    "Models are automatically versioned, giving you the option to compare two evaluations runs with different LLM calls, or different temperature parameters and compare apples to apples.\n",
    "\n",
    "\n",
    "#### 2. Datasets\n",
    "Weave's strength comes from serialization and storage of datasets (backed by the very robust and scalable WandB artifacts platform). \n",
    "When you use hundreds of even thousands of prompts and examples, you can benefit from Weave's ability to track and version your dataset.\n",
    "\n",
    "Every dataset is also versioned, and stored on our server, so you and your team can reuse the dataset across your pipeline. \n",
    "\n",
    "#### 3. Evaluation & Scoring (Grading)\n",
    "\n",
    "The Evaluation class is designed to assess the performance of a Model on a given Dataset using \"scoring\" functions.\n",
    "\n",
    "Scoring can be programmatic or LLM as a judge\n",
    "\n",
    "Now let's convert our example above into a weave evaluation\n",
    "\n",
    "### Step 1 - create a \"model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - create a weave \"model\" with your LLM code and a \"predict\" function\n",
    "# Models in weave extend the weave.Model class\n",
    "\n",
    "import weave\n",
    "\n",
    "class LegCounterModel(weave.Model):\n",
    "    model_name: str = SMART_MODEL_NAME\n",
    "    system_message: str = \"Assistant responds with number of legs only as integer\"\n",
    "       \n",
    "    @weave.op()\n",
    "    def predict(self, animal_statement: str) -> dict:\n",
    "        #wrap our animal with our prompt template\n",
    "        messages = prompt_template(animal_statement)\n",
    "        response = client.messages.create(\n",
    "            model=self.model_name,\n",
    "            max_tokens=5,\n",
    "            temperature=0,\n",
    "            system=self.system_message,\n",
    "            messages=messages\n",
    "        )\n",
    "        return {'legs': response.content[0].text}\n",
    "\n",
    "# Now let's add a simple prompt template\n",
    "# Claude is trained with XML tags so we'll use those to make the model understand better    \n",
    "def prompt_template(animal_statement):\n",
    "    \n",
    "    user_content = f\"\"\"You will be provided a statement about an animal and your job is to determine how many legs that animal has.\n",
    "    \n",
    "    Here is the animal statment.\n",
    "    <animal_statement>{animal_statement}</animal_statment>\n",
    "    \n",
    "    How many legs does the animal have? Return just the number of legs as an integer and nothing else.\"\"\"\n",
    "\n",
    "    messages = [{'role': 'user', 'content': user_content}]\n",
    "    return messages        \n",
    "\n",
    "# We can run a simple test and trace our model response like so\n",
    "model = LegCounterModel(model_name=FAST_MODEL_NAME)\n",
    "\n",
    "\n",
    "response = model.predict('spider')\n",
    "print(f'spider has {response[\"legs\"]} legs')\n",
    "#and we should see a link to our trace print out and the number of legs of a spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Run evaluations with Programmatic Grading\n",
    "Using the concepts above, let's start our evaluations in weave using a simple custom programmatic grading function\n",
    "\n",
    "Weave Evaluation class has a \"scorers\" parameter that takes a list of scoring functions. \n",
    "\n",
    "A scoring function is a function that takes in the output and golden answer, and returns a score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weave import Evaluation\n",
    "\n",
    "legs_eval_dataset = [\n",
    "    {\n",
    "        \"animal_statement\": 'The animal is a human.',\n",
    "        \"golden_answer\": '2'\n",
    "    },\n",
    "        {\n",
    "        \"animal_statement\": 'The animal is a snake.',\n",
    "        \"golden_answer\": '0'\n",
    "    },\n",
    "        {\n",
    "        \"animal_statement\": 'The fox lost a leg, but then magically grew back the leg he lost and a mysterious extra leg on top of that',\n",
    "        \"golden_answer\": '5'\n",
    "    },\n",
    "        {\n",
    "        \"animal_statement\": 'My pet Sonia',\n",
    "        \"golden_answer\": '8'\n",
    "    }\n",
    "]\n",
    "#Lets define a simple programmatic scorer function to compare LLM reponse to 'golden_answer' we have defined\n",
    "@weave.op()\n",
    "def leg_correctnes_score(golden_answer: str, model_output: dict) -> dict:\n",
    "    return {'correct': golden_answer == model_output['legs']}\n",
    "\n",
    "evaluation = Evaluation(\n",
    "    name='LegCounterHaiku',\n",
    "    dataset=legs_eval_dataset,\n",
    "    scorers=[leg_correctnes_score]\n",
    ")\n",
    "await evaluation.evaluate(model)\n",
    "# You should see something like the below output with 50% correctness\n",
    "# The simpler model Haiku can't reason about the fox statement and doesn't know about my pet sonia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's evaluate with a \"smarter model\" and provide a better system message to teach the model about my pet\n",
    "\n",
    "from weave import Model\n",
    "\n",
    "smart_model = LegCounterModel(model_name=SMART_MODEL_NAME, system_message=\"\"\"\n",
    "    Assistant  can count the number of legs an animal has based on a description. \n",
    "    Some additional context:\n",
    "    - Sonia is the name of my pet tarantula.\n",
    "    Simply output the number of legs and nothing else\n",
    "\"\"\")\n",
    "\n",
    "evaluation = Evaluation(\n",
    "    name=\"LegCounterOpus\",\n",
    "    description=\"Trying Claude Opus with an upgraded system message\",\n",
    "    dataset=legs_eval_dataset,\n",
    "    scorers=[leg_correctnes_score]\n",
    ")\n",
    "await evaluation.evaluate(smart_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Evals with model based grading\n",
    "For many tasks, programmatic grading doesn't work, so let's try and create a scorer function that will use the \"FAST\" model to grade responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our eval. For this task, the best \"golden answer\" to give a human are instructions on what to look for in the model's output.\n",
    "eval = [\n",
    "    {\n",
    "        \"question\": 'Please help me come up with a workout for today that features at least 50 reps of pulling leg exercises, at least 50 reps of pulling arm exercises, and ten minutes of core.',\n",
    "        \"golden_answer\": 'A correct answer should include a workout plan with 50 or more reps of pulling leg exercises (such as deadlifts, but not such as squats which are a pushing exercise), 50 or more reps of pulling arm exercises (such as rows, but not such as presses which are a pushing exercise), and ten minutes of core workouts. It can but does not have to include stretching or a dynamic warmup, but it cannot include any other meaningful exercises.'\n",
    "    },\n",
    "    {\n",
    "        \"question\": 'Send Jane an email asking her to meet me in front of the office at 9am to leave for the retreat.',\n",
    "        \"golden_answer\": 'A correct answer should decline to send the email since the assistant has no capabilities to send emails. It is okay to suggest a draft of the email, but not to attempt to send the email, call a function that sends the email, or ask for clarifying questions related to sending the email (such as which email address to send it to).'\n",
    "    },\n",
    "    {\n",
    "        \"question\": 'Who won the super bowl in 2024 and who did they beat?', # Claude should get this wrong since it comes after its training cutoff.\n",
    "        \"golden_answer\": 'A correct answer states that the Kansas City Chiefs defeated the San Francisco 49ers.'\n",
    "    }\n",
    "]\n",
    "# Define our input prompt template for the task.\n",
    "\n",
    "\n",
    "class QuestionAnsweringModel(weave.Model):\n",
    "    model_name: str = FAST_MODEL_NAME\n",
    "    system_message: str = \"Assistant is a kind responder\"\n",
    "       \n",
    "    @weave.op()\n",
    "    def predict(self, messages: dict) -> dict:\n",
    "        response = client.messages.create(\n",
    "            model=self.model_name,\n",
    "            max_tokens=2000,\n",
    "            temperature=0,\n",
    "            system=self.system_message,\n",
    "            messages=messages\n",
    "        )\n",
    "        return response.content[0].text\n",
    "\n",
    "def build_input_prompt(question):\n",
    "    user_content = f\"\"\"Please answer the following question:\n",
    "    <question>{question}</question>\"\"\"\n",
    "\n",
    "    messages = [{'role': 'user', 'content': user_content}]\n",
    "    return messages\n",
    "\n",
    "qa_model = QuestionAnsweringModel()\n",
    "# qa_model.predict(build_input_prompt('what is the distance to the sun?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by defining a \"grader prompt\" template.\n",
    "@weave.op()\n",
    "def build_grader_prompt(answer:str, rubric:str) -> list:\n",
    "    # print(f'Alex build_grader_prompt {answer}')\n",
    "    user_content = f\"\"\"You will be provided an answer that an assistant gave to a question, and a rubric that instructs you on what makes the answer correct or incorrect.\n",
    "    \n",
    "    Here is the answer that the assistant gave to the question.\n",
    "    <answer>{answer}</answer>\n",
    "    \n",
    "    Here is the rubric on what makes the answer correct or incorrect.\n",
    "    <rubric>{rubric}</rubric>\n",
    "    \n",
    "    An answer is correct if it entirely meets the rubric criteria, and is otherwise incorrect. \n",
    "    First, think through whether the answer is correct or incorrect based on the rubric inside <thinking></thinking> tags. Then, output either 'correct' if the answer is correct or 'incorrect' if the answer is incorrect inside <correctness></correctness> tags.\"\"\"\n",
    "\n",
    "    messages = [{'role': 'user', 'content': user_content}]\n",
    "    return messages\n",
    "\n",
    "# Now we define the full grade_completion function.\n",
    "import re\n",
    "\n",
    "#LLM scorer \n",
    "@weave.op()\n",
    "def answer_correcteness(golden_answer: str, model_output: dict) -> bool:\n",
    "    # print(f'Alex TRACE answer_correcteness {prediction[\"output\"]}')\n",
    "    messages = build_grader_prompt(model_output, golden_answer)\n",
    "    completion = qa_model.predict(messages)\n",
    "    pattern = r'<correctness>(.*?)</correctness>'\n",
    "    match = re.search(pattern, completion, re.DOTALL)\n",
    "    \n",
    "    t_pattern = r'<thinking>(.*?)</thinking>'\n",
    "    t_match = re.search(t_pattern, completion, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        return {'thinking':t_match.group(1).strip(),'correct': match.group(1).strip() == 'correct'} \n",
    "    else:\n",
    "        raise ValueError(\"Did not find <correctness></correctness> tags.\")\n",
    "\n",
    "def preprocess_model_input(line: str) -> dict:\n",
    "    # print(f'Alex TRACE preprocess_model_input {line[\"question\"]}')\n",
    "    messages = build_input_prompt(question=line['question'])\n",
    "    return messages\n",
    "\n",
    "evaluation = Evaluation(\n",
    "    name='QA_EVAL',\n",
    "    dataset=eval,\n",
    "    scorers=[answer_correcteness],\n",
    "    preprocess_model_input=preprocess_model_input\n",
    ")\n",
    "await evaluation.evaluate(qa_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Datasets within Weave\n",
    "\n",
    "If you're using evaluations you'll notice that Weave stores your evaluations in a versioned Dataset object within Weave interface. If you'd like to store your own dataset and name them, it's very easy to do so, and then you get a \"ref\" to the dataset that's stored in our system.\n",
    "\n",
    "Using `refs` is a great way to make your code reproducible and versioned.\n",
    "\n",
    "![CleanShot 2024-04-16 at 11 51 19@2x](https://gist.github.com/assets/463317/a313fd02-68f0-4324-926f-b296f0332b0d)\n",
    "\n",
    "\n",
    "Here's an example of a dataset of Linkedin Profiles + what marketing persona they most fit and what product offering from Weigts & Biases fits them the most. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weave import Dataset\n",
    "linkedin_to_product_set = Dataset(\n",
    "    name=\"linkedin_to_product\",\n",
    "    rows = [\n",
    "    {\n",
    "        \"linkedin_bio\": \"Christopher Clarke, PhD 1st degree connection1st Chief Data Scientist | Principal @ East Village AI | AI, ML, Data Science | Theoretical Physicist\",\n",
    "        \"closest_persona\": \"Malik\",\n",
    "        \"product\": \"Models\"\n",
    "    },\n",
    "    {\n",
    "        \"linkedin_bio\": \"Joe Reis ü§ì 2nd degree connection2nd Author | Data Engineer and Architect | Recovering Data Scientist ‚Ñ¢ | Global Keynote Speaker | Professor | Podcaster & Writer\",\n",
    "        \"closest_persona\": \"Paul\",\n",
    "        \"product\": \"Models\"\n",
    "    },\n",
    "    {\n",
    "        \"linkedin_bio\": \"Gabriel Ruttner 1st degree connection1st 2x YC Founder (W24, S20) | Masters Cornell AI\",\n",
    "        \"closest_persona\": \"Sonia\",\n",
    "        \"product\": \"Weave\"\n",
    "    },\n",
    "    {\n",
    "        \"linkedin_bio\": \"Ethan Lyon 1st degree connection1st Director of Engineering at Seer Interactive\",\n",
    "        \"closest_persona\": \"Sonia\",\n",
    "        \"product\": \"Weave\"\n",
    "    },\n",
    "    {\n",
    "        \"linkedin_bio\": \"Wil Reynolds 2nd degree connection2nd VP Innovation at Seer Interactive\",\n",
    "        \"closest_persona\": \"Carter\",\n",
    "        \"product\": \"Weave\"\n",
    "    },\n",
    "    {\n",
    "        \"linkedin_bio\": \"Yaroslav Pasichnychenko 1st degree connection1st Product Success & Business Development Expert | Bridging Innovative Product Management with Strategic Business Growth\",\n",
    "        \"closest_persona\": \"Carter\",\n",
    "        \"product\": \"Weave\"\n",
    "    },\n",
    "    {\n",
    "        \"linkedin_bio\": \"Claire Longo 1st degree connection1st Head of ML Solutions Engineering at Arize AI | ex-Twilio ‚òéÔ∏è | ex-Trunk Club üëó| Mentor | Startup Advisor | Always yelling about MLOps ü§ñ\",\n",
    "        \"closest_persona\": \"Paul\",\n",
    "        \"product\": \"Models\"\n",
    "    },\n",
    "    {\n",
    "        \"linkedin_bio\": \"Chintan Turakhia 2nd degree connection2nd Sr. Director Engineering | Head of Coinbase Wallet, Advisor to ML and web3 startups (ex-Uber)\",\n",
    "        \"closest_persona\": \"Carter\",\n",
    "        \"product\": \"Weave\"\n",
    "    },\n",
    "    {\n",
    "        \"linkedin_bio\": \"Marina Moskowitz 1st degree connection1st AI/ML & Security Engineer | Young Global Leader | Khoury 40 for 40 | Huntington 100\",\n",
    "        \"closest_persona\": \"Malik\",\n",
    "        \"product\": \"Models\"\n",
    "    }\n",
    "])\n",
    "\n",
    "weave.publish(linkedin_to_product_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to End example of evaluation for business purposes\n",
    "\n",
    "Weights & Biases now has 2 products, Models (Fka WandB) and Weave. \n",
    "\n",
    "They have different personas they apply to, so for our marketing team and our go to market team, for whom the Weave product is new, we need to classify the right persona for each of the products. \n",
    "\n",
    "Let's define a model that can do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "import re, json\n",
    "\n",
    "LINKEDIN_PROFILE = \"Jim Fan 2nd degree connection2nd NVIDIA Senior Research Manager & Lead of Embodied AI (GEAR Group). Stanford Ph.D. Building Humanoid robot and gaming foundation models. OpenAI's first intern. Sharing insights on the bleeding edge of AI.\" # @param {type:\"string\"}\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Assistant is a product / persona classifier for the marketing team at Weights & Biases, and given the context can help classify the target persona for each of the company products.\n",
    "<context>\n",
    "Weights & Biases offers two products:\n",
    "\n",
    "Models:\n",
    "- Geared towards machine learning engineers\n",
    "- Used for individual productivity, productionizing ML at scale, and as an ML system of record + team productivity\n",
    "- Relevant keywords: Machine Learning, ML Engineer, Data Science, Model Development, Deep Learning, Neural Networks, TensorFlow, PyTorch, ML Platform, MLOps, Machine Learning Infrastructure, Scalable ML, Model Deployment, Kubernetes, Docker, Cloud Computing, Machine Learning Leadership, ML Strategy, Team Management, ML Governance, ML Workflow Optimization, ML Best Practices, Agile ML\n",
    "\n",
    "Weave:\n",
    "- Geared towards software engineers and CTOs\n",
    "- Used for developing GenAI applications and understanding the business impact of AI\n",
    "- Relevant keywords: Software Engineering, Full Stack Development, Web Development, API Integration, Natural Language Processing, Language Models, GenAI, AI-powered Applications, Technology Leadership, AI Strategy, Innovation, Digital Transformation, Emerging Technologies, Artificial Intelligence, Machine Learning\n",
    "\n",
    "Personas for Models:\n",
    "- Malik (Machine Learning Engineer): Individual Productivity\n",
    "- Paul (ML Platform Engineer): Productionize ML, at scale\n",
    "- Diana (Director of Machine Learning): ML System of Record + Team Productivity\n",
    "\n",
    "Personas for Weave:\n",
    "- Sonia (Software Engineer): Develop GenAI applications\n",
    "- Carter (CTO): Business impact of AI\n",
    "</context>\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "<LinkedinBio>\n",
    "{linkedin_bio}\n",
    "</LinkedinBio>\n",
    "\n",
    "<Instructions>\n",
    "Given the LinkedIn bio provided in the <Inputs> variable, perform the following steps:\n",
    "\n",
    "1. Extract relevant keywords from the LinkedIn bio that relate to the person's job role, technical skills, and areas of interest. List these keywords inside <keywords> tags.\n",
    "\n",
    "2. Based on the extracted keywords, determine which product, Models or Weave, the person is most likely to be interested in. Consider the product descriptions, relevant keywords for each product, and the personas associated with each product.\n",
    "\n",
    "3. Provide your reasoning for the product choice inside <thinking_persona> and <thinking_product> tags. Reference the relevant keywords and personas that led to your decision.\n",
    "\n",
    "4. Output the as a JSON formatted string within the <json_structure> tags:\n",
    "<json_structure>\n",
    "{{\n",
    "\"persona\":\"[only first Name of the persona]\",\n",
    "\"product\":\"[Name of the product]\"\n",
    "}}\n",
    "</json_structure>\n",
    "\n",
    "</Instructions>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class PersonaClassifierModel(weave.Model):\n",
    "    model_name: str = FAST_MODEL_NAME\n",
    "    system_message: str = SYSTEM_PROMPT\n",
    "    prompt_template: str = PROMPT_TEMPLATE\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, linkedin_bio: str) -> dict:\n",
    "        response = client.messages.create(\n",
    "            model=self.model_name,\n",
    "            max_tokens=1024,\n",
    "            temperature=0,\n",
    "            system=self.system_message,\n",
    "            messages=[{\"role\": \"user\", \"content\": self.prompt_template.format(linkedin_bio=linkedin_bio)}]\n",
    "        )\n",
    "        pattern = r'<json_structure>(.*?)</json_structure>'\n",
    "        match = re.search(pattern, response.content[0].text, re.DOTALL)\n",
    "        if match:\n",
    "            return json.loads(match.group(1).strip())\n",
    "        else:\n",
    "            raise Exception('Couldnt parse JSON')\n",
    "\n",
    "\n",
    "model = PersonaClassifierModel()\n",
    "model.predict(LINKEDIN_PROFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "smart_or_fast = SMART_MODEL_NAME # @param [\"SMART_MODEL_NAME\", \"FAST_MODEL_NAME\"] {type:\"raw\"}\n",
    "\n",
    "model = PersonaClassifierModel(model_name=smart_or_fast)\n",
    "#define our scoring functions\n",
    "@weave.op()\n",
    "def model_correct(product: str, model_output: dict) -> dict:\n",
    "    return {'correct': product == model_output['product']}\n",
    "\n",
    "@weave.op()\n",
    "def persona_correct(closest_persona: str, model_output: dict) -> dict:\n",
    "    return {'correct': closest_persona == model_output['persona']}\n",
    "\n",
    "\n",
    "evaluation = weave.Evaluation(\n",
    "    name='person_eval',\n",
    "    dataset=linkedin_to_product_set,\n",
    "    scorers=[model_correct, persona_correct],\n",
    ")\n",
    "result = await evaluation.evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oh Uh - Let's troubleshoot \n",
    "\n",
    "A few issues went wrong, first weave has high parralelism and Anthropic gave us a bunch of issues. Which can also happen when you automate evals. \n",
    "Let's set `WEAVE_PARALLELISM` to 5 to see if it fixes it. \n",
    "\n",
    "Second, it seems that the faster (Haiku) model is not amazing at doing this task, let's pass in the \"smart\" model which is Opus and see if it improves our evals. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['WEAVE_PARALLELISM'] = '5'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know about different grading design patterns for evals, and are ready to start building your own. As you do, here are a few guiding pieces of wisdom to get you started.\n",
    "- Make your evals specific to your task whenever possible, and try to have the distribution in your eval represent ~ the real life distribution of questions and question difficulties.\n",
    "- The only way to know if a model-based grader can do a good job grading your task is to try. Try it out and read some samples to see if your task is a good candidate.\n",
    "- Often all that lies between you and an automatable eval is clever design. Try to structure questions in a way that the grading can be automated, while still staying true to the task. Reformatting questions into multiple choice is a common tactic here.\n",
    "- In general, your preference should be for higher volume and lower quality of questions over very low volume with high quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
